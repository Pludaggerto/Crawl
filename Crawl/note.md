# 前言
- 最基础的爬虫操作是使用urllib与requests等解析页面
- 之后可以使用页解析库、数据库处理等更加方便的爬虫手段
- 使用Ajax、动态渲染页面、验证码、模拟登录等相关处理办法绕过平台的验证。
- 如何使用pyspider，Scrapy等爬虫框架
# 1 开发环境配置
- 介绍了各种环境的安装
# 2 爬虫基础
## 2.1 HTTP基本原理
 - 请求的文档类型为 document，代表我们这次请求的是一个HTML文档，内容就是一些 HTML 代码。
## 2.5 代理的基本原理
- 目的：防止IP被封
# 3 基本库的使用
## 3.1 urllib
- urllib包含四个模块
	- request: 模拟发送请求
	- error： 异常处理
	- parse: URL处理方法
	- robotparser：识别网站是否可以爬
## 3.2 request
request库拥有更多的细节操作
# 4 解析库的使用
## 4.1 Xpath
所以说， 如果要想获取子孙节点内部的所有文本， 可以直接用//加 text()的方式， 这样可以保证
获取到最全面的文本信息， 但是可能会夹杂一些换行符等特殊字符。 如果想获取某些特定子孙节点下
的所有文本， 可以先选取到特定的子孙节点， 然后再调用 text()方法获取其内部文本， 这样可以保证
获取的结果是整洁的
## 